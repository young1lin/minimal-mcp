# LLM
ç®€å•ä»‹ç»ä¸€ä¸‹ LLMï¼ˆLarge Language Modelï¼‰ï¼Œå°±æ˜¯ä½ è¾“å…¥ä¸€æ®µæ–‡å­—ï¼Œæœºå™¨çŒœä½ ä¸‹ä¸€ä¸ªè¯æ˜¯ä»€ä¹ˆï¼Œå°±è¿™ä¹ˆç®€å•ã€‚
è¯¦ç»†äº†è§£ï¼Œçœ‹æˆ‘ä¸Šä¸€ç¯‡æ–‡ç« ï¼ˆTODO æ”¾é“¾æ¥ï¼‰ï¼Œå¤§æ¨¡å‹å¹¶æ²¡æœ‰â€œè®°ä½â€ä½ çš„äº‹æƒ…ï¼Œçœ‹ä¸‹é¢æ¼”ç¤ºã€‚

è¿™é‡Œåšä¸€äº›éå¸¸ç®€å•çš„è°ƒç”¨

## å•ä¸€è°ƒç”¨

è¯·æ±‚
```http
POST https://api.deepseek.com/chat/completions HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: Bearer {{$dotenv DEEPSEEK_API_KEY}}

{
  "messages": [
    {
      "content": "You are an intelligent customer service agent named Alice. Your main role is to help users answer their questions.",
      "role": "system"
    },
    {
      "content": "Hello, who are you?",
      "role": "user"
    }
  ],
  "model": "deepseek-chat",
  "stream": false,
  "temperature": 0
}
```

response body
```json
{
  "id": "e479b679-84d4-48c1-bf6f-ad7f56c87682",
  "object": "chat.completion",
  "created": 1756429739,
  "model": "deepseek-chat",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! I'm Alice, your intelligent customer service agent. How can I assist you today?"
      },
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 31,
    "completion_tokens": 19,
    "total_tokens": 50,
    "prompt_tokens_details": {
      "cached_tokens": 0
    },
    "prompt_cache_hit_tokens": 0,
    "prompt_cache_miss_tokens": 31
  },
  "system_fingerprint": "fp_feb633d1f5_prod0820_fp8_kvcache"
}
```

## å¤šè½®å¯¹è¯

```http
POST https://api.deepseek.com/chat/completions HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: Bearer {{$dotenv DEEPSEEK_API_KEY}}

{
  "messages": [
    {
      "content": "You are an intelligent customer service agent named Alice. Your main role is to help users answer their questions. The company's main business is quantitative trading.",
      "role": "system"
    },
    {
      "content": "Hello, who are you?",
      "role": "user"
    },
    {
      "content": "Hello! I'm Alice, your intelligent customer service agent. How can I assist you today?",
      "role": "assistant"
    },
    {
      "content": "I would like to inquire about your company's business.",
      "role": "user"
    }
  ],
  "model": "deepseek-chat",
  "stream": false,
  "temperature": 0
}
```
response body
```json
{
  "id": "f21a4cd8-baad-449c-a91a-8ac38a674715",
  "object": "chat.completion",
  "created": 1756429876,
  "model": "deepseek-chat",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Of course! Our company specializes in quantitative trading, which involves using mathematical models, algorithms, and data analysis to make trading decisions in financial markets. We leverage technology and data to identify patterns, manage risk, and execute trades efficiently. \n\nIs there a specific aspect of quantitative trading you'd like to learn more about?"
      },
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 74,
    "completion_tokens": 63,
    "total_tokens": 137,
    "prompt_tokens_details": {
      "cached_tokens": 0
    },
    "prompt_cache_hit_tokens": 0,
    "prompt_cache_miss_tokens": 74
  },
  "system_fingerprint": "fp_feb633d1f5_prod0820_fp8_kvcache"
}
```

## ä½¿ç”¨ Function Calling

è¯·æ±‚
```http
POST https://api.deepseek.com/chat/completions HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: Bearer {{$dotenv DEEPSEEK_API_KEY}}

{
  "messages": [
    {
      "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å®¢æœ Aliceï¼Œä½ çš„ä¸»è¦ä½œç”¨å°±æ˜¯å¸®ç”¨æˆ·è§£ç­”ç–‘é—®",
      "role": "system"
    },
    {
      "content": "ä½ å¥½ï¼ŒæŸ¥ä¸€ä¸‹åŒ—äº¬çš„å¤©æ°”",
      "role": "user"
    }
  ],
  "model": "deepseek-chat",
  "tools": [
    {
      "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get weather of a location, the user should supply a location first.",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA"
                    }
                },
                "required": ["location"]
            }
        }
    }
  ],
  "stream": false,
  "temperature": 0
}
```

response body
```json
{
  "id": "2e85bf30-d509-4ee6-bc18-b65d1b8e02df",
  "object": "chat.completion",
  "created": 1756430135,
  "model": "deepseek-chat",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "I'll check the weather in Hangzhou for you right away.",
        "tool_calls": [
          {
            "index": 0,
            "id": "call_0_c9f112b0-766e-48ee-8b7d-a70c14f16b43",
            "type": "function",
            "function": {
              "name": "get_weather",
              "arguments": "{\"location\": \"hangzhou\"}"
            }
          }
        ]
      },
      "logprobs": null,
      "finish_reason": "tool_calls"
    }
  ],
  "usage": {
    "prompt_tokens": 212,
    "completion_tokens": 28,
    "total_tokens": 240,
    "prompt_tokens_details": {
      "cached_tokens": 192
    },
    "prompt_cache_hit_tokens": 192,
    "prompt_cache_miss_tokens": 20
  },
  "system_fingerprint": "fp_feb633d1f5_prod0820_fp8_kvcache"
}
```

### Function Calling conversation

request

```http
POST https://api.deepseek.com/chat/completions HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: Bearer {{$dotenv DEEPSEEK_API_KEY}}

{
  "messages": [
    {
      "role": "system",
      "content": "You are an intelligent customer service agent named Alice. Your main role is to help users answer their questions."
    },
    {
      "role": "user",
      "content": "Hello, please check the weather in Beijing."
    },
    {
      "role": "assistant",
      "content": "I'll check the weather in Hangzhou for you right away.",
      "tool_calls": [
        {
          "index": 0,
          "id": "call_0_c9f112b0-766e-48ee-8b7d-a70c14f16b43",
          "type": "function",
          "function": {
            "name": "get_weather",
            "arguments": "{\"location\": \"hangzhou\"}"
          }
        }
      ]
    },
    {
      "role": "tool",
      "tool_call_id": "call_0_c9f112b0-766e-48ee-8b7d-a70c14f16b43",
      "content": "Sunny, 29Â°C"
    }
  ],
  "model": "deepseek-chat",
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get weather information for a location.",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The name of the city to get weather for. Only support low case location name, like beijing, shanghai, hangzhou, newyork"
            }
          },
          "required": [
            "location"
          ]
        }
      }
    }
  ],
  "stream": false,
  "temperature": 0
}
```

response body

```json
{
  "id": "2ef2e44e-8540-4798-8530-77b63209e1a9",
  "object": "chat.completion",
  "created": 1756430434,
  "model": "deepseek-chat",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "The weather in Hangzhou is currently sunny with a temperature of 29Â°C. It's a beautiful day there! Is there anything else you'd like to know about the weather or any other assistance I can provide?"
      },
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 238,
    "completion_tokens": 44,
    "total_tokens": 282,
    "prompt_tokens_details": {
      "cached_tokens": 0
    },
    "prompt_cache_hit_tokens": 0,
    "prompt_cache_miss_tokens": 238
  },
  "system_fingerprint": "fp_feb633d1f5_prod0820_fp8_kvcache"
}
```

è¿™å°±æ˜¯å¤§æ¨¡å‹ï¼Œå°±æ˜¯çŒœä½ ä¸‹ä¸€æ®µè¯æ˜¯ä»€ä¹ˆï¼Œå°±è¿™ä¹ˆç®€å•ã€‚

# èƒŒæ™¯ä»‹ç»

ä¸€å¼€å§‹ OpenAI çš„ GPT æ¥å£å‡ºæ¥çš„æ—¶å€™ï¼Œå¹¶æ²¡æœ‰æä¾› Function Calling è¿™æ ·çš„åŠŸèƒ½ï¼Œä½†æ˜¯ä¸ºäº†å’Œç°å®ä¸–ç•Œè¿›è¡Œäº¤äº’ï¼ŒLangChain, Smolagents è¿™ç§æ¡†æ¶ï¼Œåˆæƒ³è¦å’Œ
ç°å®ä¸–ç•Œè¿›è¡Œäº¤äº’ï¼Œå¹¶è·å¾—ç›¸åº”çš„ä¿¡æ¯ï¼Œå®ƒå°±åªèƒ½åœ¨è¿”å›çš„æ–‡æœ¬ä¸Šï¼Œè‡ªå®šä¹‰æ ¼å¼ï¼Œé€šè¿‡è£…é¥°å™¨ @toolï¼ˆJava é‡Œå¯ä»¥ç”¨æ³¨è§£ï¼‰è§£ææˆä¸€ä¸ª Tool å¯¹è±¡ï¼Œè¿™ä¸ªå¯¹è±¡æœ‰å…¥å‚ï¼ŒåŠå…¶å‚æ•°æè¿°ï¼Œ
æœ‰è¿™ä¸ªæ–¹æ³•çš„æè¿°ã€‚åœ¨è°ƒç”¨å¤§æ¨¡å‹çš„æ—¶å€™ï¼Œåªéœ€è¦ Tool è§£ææˆ System prompt æ–‡æœ¬ï¼Œå¹¶ä¸”ä»¥ç‰¹å®šçš„æ ¼å¼è¿”å›ï¼Œè¿›è¡Œæ˜¯å¦è°ƒç”¨å·¥å…·ï¼Œç„¶åæ‰§è¡Œä¸‹ä¸€æ­¥ã€‚è¿™é‡Œæˆ‘ä»¬æš‚ä¸”æŒ‰ä¸‹ä¸è¡¨ï¼Œ
åé¢ä¼šä»‹ç» Cline æ˜¯å¦‚ä½•å®ç°çš„ï¼Œè¯¦ç»†è§£é‡Šé€šè¿‡æ§åˆ¶ system prompt æ¥å®ç°ç»“æ„åŒ–è¿”å›ï¼Œå¹¶ä¸”è§£æç»“æ„åŒ–è¿”å›å†…å®¹å®ç°â€œæ™ºèƒ½â€è°ƒç”¨å·¥å…·ã€‚

è¿™æ ·è¯´æœ‰ç‚¹æŠ½è±¡ï¼Œå†™ä¸€ä¸ªä¾‹å­ï¼Œè¿™é‡Œæˆ‘æ˜¯ç”¨ uv æ¥ç®¡ç†åŒ…ã€‚æˆ‘å¹¶ä¸æ¨èä½¿ç”¨ LangChain ç”¨äºç”Ÿäº§ç¯å¢ƒï¼Œå˜æ›´å¤ªå¤šäº†ï¼Œæœ¬æ¥å°±æ˜¯ä¸€ä»¶ç®€å•çš„äº‹æƒ…ï¼Œè¶Šæè¶Šå¤æ‚ï¼Œå·²ç»è¿‡äºè‡ƒè‚¿äº†ï¼Œ
LangChain Communityï¼Œæ–‡æ¡£ä¹Ÿæ›´æ–°ä¸åŠæ—¶ï¼Œå¯¹æ–°äººæ¥è¯´å¾ˆä¸å‹å¥½ï¼Œä»€ä¹ˆ LangSmithï¼ŒLangGraph å¹²ä»€ä¹ˆå‘¢ã€‚

## Python part

prepare install uv, langchain, langchain-openai, python-dotenv.

```shell
python -m pip install uv
uv init langchain-weather
cd langchain-weather && uv add langchain langchain-openai python-dotenv

# replace this api_key with your own
echo "DEEPSEEK_API_KEY=your_deepseek_api_key_here" > .env
```

main.py
```python
import os
import asyncio
import warnings

# ç¦ç”¨ LangSmith è¿½è¸ªå’Œè­¦å‘Š
os.environ["LANGCHAIN_TRACING_V2"] = "false"
warnings.filterwarnings("ignore", category=UserWarning, module="langsmith")

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.agents import create_react_agent, AgentExecutor
from langchain.prompts import ChatPromptTemplate
from langchain.tools import tool

load_dotenv()

api_key = os.getenv("DEEPSEEK_API_KEY")
if not api_key:
    raise ValueError("DEEPSEEK_API_KEY not found in .env file")

llm = ChatOpenAI(
    model="deepseek-chat",
    base_url="https://api.deepseek.com/v1",
    api_key=api_key,
    streaming=True,
    temperature=0.1,
)


@tool
def get_weather(location: str) -> str:
    """
    Get weather information for a location.

    Args:
        location (str): The name of the city to get weather for. Only support low case location name, like beijing, shanghai, hangzhou, newyork'.

    Returns:
        str: Weather information for the specified location.
    """
    match location:
        case "beijing":
            return "åŒ—äº¬ä»Šæ—¥å¤©æ°”ï¼šæ™´å¤©ï¼Œæ°”æ¸© 25Â°Cï¼Œæ¹¿åº¦ 45%ï¼Œå¾®é£"
        case "shanghai":
            return "ä¸Šæµ·ä»Šæ—¥å¤©æ°”ï¼šå¤šäº‘ï¼Œæ°”æ¸© 28Â°Cï¼Œæ¹¿åº¦ 55%ï¼Œå¾®é£"
        case "hangzhou":
            return "æ­å·ä»Šæ—¥å¤©æ°”ï¼šå°é›¨è½¬å¤§é›¨ï¼Œæ°”æ¸© 22Â°C åˆ° 28Â°Cï¼Œæ¹¿åº¦ 70%ï¼Œå¾®é£"
        case "newyork":
            return "çº½çº¦ä»Šæ—¥å¤©æ°”ï¼šå°é›¨ï¼Œæ°”æ¸© 15Â°Cï¼Œæ¹¿åº¦ 40%ï¼Œå¾®é£"
        case _:
            return f"æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•æŸ¥è¯¢åˆ° '{location}' çš„å¤©æ°”ä¿¡æ¯ã€‚ç›®å‰ä»…æ”¯æŒæŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”ã€‚"


# ReAct æç¤ºæ¨¡æ¿
from langchain import hub

prompt = hub.pull("hwchase17/react")

# åˆ›å»º ReAct agent
agent = create_react_agent(llm, [get_weather], prompt)
agent_executor = AgentExecutor(agent=agent, tools=[get_weather], verbose=False)


async def main():
    user_input = "æŸ¥è¯¢çº½çº¦å¤©æ°”"
    print(f"ç”¨æˆ·è¾“å…¥: {user_input}\n")

    try:
        # ä½¿ç”¨ astream_events è·å–çœŸæ­£çš„æµå¼è¾“å‡º
        current_content = ""
        
        async for event in agent_executor.astream_events(
            {"input": user_input}, 
            version="v1"
        ):
            kind = event["event"]
            
            # æµå¼è¾“å‡º LLM å†…å®¹
            if kind == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                if content:
                    current_content += content
                    print(content, end="", flush=True)
                    
            # å·¥å…·å¼€å§‹æ‰§è¡Œæ—¶
            elif kind == "on_tool_start":
                if event["name"] == "get_weather":
                    # ä»å½“å‰ç´¯ç§¯çš„å†…å®¹ä¸­æå–å·¥å…·å‚æ•°
                    lines = current_content.split('\n')
                    action_input = ""
                    for line in lines:
                        if "Action Input:" in line:
                            action_input = line.split("Action Input:")[-1].strip()
                            break
                    print(f"\n\nğŸ”§ å¼€å§‹è°ƒç”¨å·¥å…·: {event['name']}")
                    print(f"   è¾“å…¥å‚æ•°: {action_input}")
                    
            # å·¥å…·æ‰§è¡Œå®Œæˆæ—¶
            elif kind == "on_tool_end":
                if event["name"] == "get_weather":
                    tool_output = event["data"].get("output", "")
                    print(f"\nğŸ“ å·¥å…·è¿”å›ç»“æœ: {tool_output}\n")
                
        print(f"\n\n{'-'*50}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    asyncio.run(main())
```

execute main.py

```shell
uv run main.py
```

response
```plaintext
ç”¨æˆ·è¾“å…¥: æŸ¥è¯¢çº½çº¦å¤©æ°”

Thought: The user is asking for the weather in New York. I need to use the get_weather function to retrieve this information. The function requires the location name in lowercase, so I should use "newyork".

Action: get_weather
Action Input: newyork

ğŸ”§ å¼€å§‹è°ƒç”¨å·¥å…·: get_weather
   è¾“å…¥å‚æ•°: newyork

ğŸ“ å·¥å…·è¿”å›ç»“æœ: çº½çº¦ä»Šæ—¥å¤©æ°”ï¼šå°é›¨ï¼Œæ°”æ¸© 15Â°Cï¼Œæ¹¿åº¦ 40%ï¼Œå¾®é£

I now know the final answer

Final Answer: çº½çº¦ä»Šæ—¥å¤©æ°”ï¼šå°é›¨ï¼Œæ°”æ¸© 15Â°Cï¼Œæ¹¿åº¦ 40%ï¼Œå¾®é£
--------------------------------------------------
```

å¯ä»¥ä»ä¸Šé¢çœ‹å‡ºæ¥ï¼Œå¯ä»¥é€šè¿‡ LangChain è¿™æ ·çš„æ¡†æ¶ï¼Œæ˜¯å¯ä»¥é€šè¿‡è°ƒç”¨ â€œä»£ç â€ æ¥è¿”å›å®æ—¶çš„å†…å®¹åŠ å…¥åˆ°å¯¹è¯ä¸­ã€‚è¿™é‡Œç”¨åˆ°çš„æ˜¯æ¯”è¾ƒæ—©æœŸçš„ ReAct å½¢å¼ï¼Œé€šè¿‡
ç¼–å†™åˆé€‚çš„ç³»ç»Ÿæç¤ºè¯ï¼Œæ¥å†³å®šè°ƒç”¨ä»€ä¹ˆå·¥å…·ï¼Œä¸‹ä¸€æ­¥å¦‚ä½•æ‰§è¡Œã€‚åœ¨ä¸Šä¸€èŠ‚å±•ç¤ºçš„ Function Calling æ˜¯æ¯”è¾ƒåé¢æ‰å‡ºçš„ï¼Œæˆ‘ 23 å¹´åˆšåšè¿™å—çš„æ—¶å€™ï¼ŒOpenAI è¿˜æ²¡æœ‰
æ¨å‡º Function Callingã€‚

ä¸¤è€…å‘¢ï¼Œæœ¬è´¨ä¸Šï¼Œéƒ½æ˜¯é€šè¿‡è°ƒç”¨é¡¹å†…çš„æ–¹æ³•/å‡½æ•°æ¥å®ç°å’Œç°å®ä¸–ç•Œäº¤äº’ï¼Œè·å–æœ€æ–°çš„ä¿¡æ¯ï¼Œå¹¶ä¸”è¿”å›ç»™å¤§æ¨¡å‹ï¼Œè®©å¤§æ¨¡å‹ç»§ç»­çŒœä¸‹ä¸€ä¸ª Tokenï¼Œä¸‹ä¸‹ä¸€ä¸ª Token æ˜¯ä»€ä¹ˆï¼Œç„¶åè¿”å›ç»™ä½ 
çš„ä¸€ä¸ªè¿‡ç¨‹ã€‚

ä½  Python æœ‰ LangChainï¼ŒJava æœ‰ Spring AIï¼ŒåŸºäº TypeScript å†™çš„ AI å®¢æˆ·ç«¯æœ‰ Clineï¼Œæœ‰ Continueï¼Œæœ‰ Claude Desktopï¼Œæœ‰ Copilotï¼Œæ¯ä¸ªè¯­è¨€ï¼Œæ¯ä¸ªæ¡†æ¶éƒ½æœ‰è‡ªå·±çš„å®ç°ï¼Œé‚£æˆ‘æƒ³å®ç°ä¸€ä¸ª
è·å–å½“å‰å¤©æ°”ï¼Œæˆ–è€…æ“ä½œ Redisï¼ŒMongoDB æ’å…¥æ•°æ®ï¼Œæ›´æ–°æ•°æ®è¿™äº› toolï¼ˆå·¥å…·ï¼‰æ€ä¹ˆåŠï¼Ÿæ¯ä¸ªå®¢æˆ·ç«¯éƒ½å†™ä¸€éï¼Ÿé‡å¤é€ è½®å­ï¼Œè¿˜è¦æ”¹å®ƒä»¬å¯¹åº”çš„æºç ï¼Œè¿™æ ·å¯¹ä¸äº†è§£å¤§æ¨¡å‹ï¼Œä¸äº†è§£
ä»£ç çš„äººæ¥è¯´ä¸å¤ªå‹å¥½ï¼Œè€Œä¸”éƒ½æ˜¯é‡å¤çš„å·¥ä½œï¼Œæ¯ä¸ªè¯­è¨€éƒ½å®ç°ä¸€éï¼Œä¸å¥½ã€‚

è¿™ä¸ªæ—¶å€™ï¼Œä¹Ÿå°±æ˜¯ 2024 å¹´ 11 æœˆï¼ŒClaude ç‰µå¤´æå‡ºäº† MCP Model Context Protocol ä¸€ä¸ªæ¦‚å¿µï¼Œæˆ‘ä»¬å…ˆä¸ç”¨ç®¡é‡Œé¢çš„å…¶ä»–ç»„ä»¶ï¼Œä¾‹å¦‚ prompts, resources, å¦‚ä½•æ¡æ‰‹ï¼ŒJSON RPC è¿™äº›ä¸œè¥¿ã€‚
ä½ åªéœ€è¦çŸ¥é“ï¼ŒMCP å°±æ˜¯ä¸ºäº†è§£å†³é‡å¤é€ è½®å­ï¼Œå¤§å®¶åªè¦é…ç½®ç›¸åº”çš„å†…å®¹ï¼Œå°±å¯ä»¥ç›´æ¥è°ƒç”¨è¿™äº›å°è£…å¥½çš„ tools å°±è¡Œäº†ã€‚Claude è®©å¤§å®¶éƒ½æ¥æ¥è¿™ä¸ªåè®®ï¼Œè‡³äºä½ æ€ä¹ˆè°ƒç”¨
æ€ä¹ˆåœ¨ IPCï¼ˆInternet Process Communicationï¼‰ ä¸­åå•†åœ¨åè®®é‡Œé¢éƒ½å†™äº†ã€‚

ç›¸ä¿¡ä½ æˆ–å¤šæˆ–å°‘ç”¨è¿‡ MCPï¼Œæˆ–è€…å¬è¿‡ï¼Œå°±ä¸‹é¢è¿™äº›é…ç½®å°±èƒ½è®© LLM å®¢æˆ·ç«¯è‡ªåŠ¨è°ƒç”¨è¿™äº›å·¥å…·ï¼Œåƒä¸‹é¢è¿™æ ·é…ç½®ã€‚

```json
{
  "mcpServers": {
    "figma-mcp": {
      "command": "npx",
      "args": ["figma-mcp"],
      "env": {
        "FIGMA_API_KEY": "<YOUR_API_KEY>"
      }
    }
  }
}
```

æˆ‘å°±è¿™ä¹ˆä¸€é…ç½®ï¼Œè¯¶ï¼Œæˆ‘å°±èƒ½å®ç°é€šè¿‡ Copilot æˆ–è€… Cursor æˆ–è€…ä»»ä½•ä¸€ä¸ªæ”¯æŒ MCP çš„å®¢æˆ·ç«¯ï¼Œå°±ç”¨è‡ªç„¶è¯­è¨€æè¿°ï¼Œå°±èƒ½è°ƒç”¨è¿™ä¸ªå·¥å…·ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä¼šé€šè¿‡ä»é›¶å¼€å§‹ï¼Œåªè°ƒç”¨æœ€åŸºç¡€çš„åº“ï¼Œä»»ä½•è¯­è¨€çš„ HTTP åº“æ¥å®ç° MCP Clientï¼ŒServerï¼ŒHostã€‚å¹¶ä¸”æ¥å…¥ Cursorï¼ŒClineï¼ŒCopilotï¼ŒQwen Coder è¿™äº›å·¥å…·ã€‚
å¾ˆç®€å•ï¼Œä¸€æ­¥æ­¥æ¥ï¼Œä½ ä¹Ÿæ˜¯äº†è§£ MCPï¼ŒLLM çš„ â€œä¸“å®¶â€ã€‚

# å®ç° MCP Client

## Python

## Java

## Go

## Node.js